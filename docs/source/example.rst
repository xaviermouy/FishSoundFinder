Tutorial
========

This tutorial goes through all the steps required to set up a project, run FishSound Finder and visualize the detections results in Raven and PAMlab.

Setting up the project structure
--------------------------------

* Create a new folder for your project called *Tutorial*
* In the *Tutorial* folder, create with three new subfolders called *model*, *data*, and *results*.

The project folder should look like this:

.. image:: _static/tutorial_folder_structure.png
   :scale: 90 %



Downloading the model and configuration file
--------------------------------------------

In this example we will run the generic fish detector for British-Columbia. The configuration file and classification model are located on 
FishSound Finder's GitHub page under the `british-columbia_generic_config <https://github.com/xaviermouy/FishSound_Finder/tree/master/models/british-columbia_generic_config>`_ directory.

.. image:: _static/github_models.png
   :scale: 70 %
 
* Download the classification model `RF50_model_20201208T223420.sav <https://raw.githubusercontent.com/xaviermouy/FishSound_Finder/master/models/british-columbia_generic_config/RF50_model_20201208T223420.sav>`_ from GitHub.
* Download the configuration file `config.yaml <https://raw.githubusercontent.com/xaviermouy/FishSound_Finder/master/models/british-columbia_generic_config/config.yaml>`_ from GitHub..
* Place both files in your *model* folder.


Downloading audio data
----------------------

For this tutorial we will use a single wav file located in the *example* directory of `FishSound Finder's GitHub page <https://github.com/xaviermouy/FishSound_Finder/tree/master/example>`_. 
This audio file was collected off Vancouver Island by a SoundTrap recorder set with a sampling rate of 48 kHz. The original file was 30-minute long, but
was trimmed down to 1 minute for this tutorial. Note that FishSound Finder can interpret timestamp information from the name of the audio files.
It currently supports timestamp formats from the SoundTrap and AMAR audio recorders. 

* Download the audio file `67674121.181017060806.wav <https://github.com/xaviermouy/FishSound_Finder/raw/master/example/67674121.181017060806.wav>`_ from GitHub.
* Place it in your *data* folder.

Creating a deployment file
--------------------------

Creating a deployment file is not necessary but highly recommended. It allows FishSound Finder to embed metadata into the detection results, which
makes the post-processing of detection results from multiple recorders much easier. Notice that metadata are included in the NetCDF files generated by FishSound Finder, but
not in the Raven or PAMlab files.

* Download a deployment file template `deployment_info.csv <https://raw.githubusercontent.com/xaviermouy/FishSound_Finder/master/example/deployment_info.csv>`_ from GitHub.
* Place it in your *data* folder.
* Open the deployment_info.csv file with a text editor (e.g. NotePad++) and edit the fields as necessary.

	* **audio_channel_number**: *[Integer (starting at 1)]* Audio channel to being processed.
	* **UTC_offset**: *[Integer]* GMT hour offset of the timestamps in the audio filenames.
	* **sampling_frequency**: *[Float]* Sampling frequency of the audio files in Hz.
	* **bit_depth**: *[Integer]* Bit depth of the audio files.
	* **mooring_platform_name**: *[String]* Short name describing the mooring design (e.g. volumetric array, bottom-mounted).
	* **recorder_type**: *[String]* Name of the acoustic recorder (e.g. SoundTrap 300, AMAR G3).
	* **recorder_SN**: *[String]* Serial number of the acoustic recorder.
	* **hydrophone_model**: *[String]* Model of the hydrophone (e.g. HTI-96).
	* **hydrophone_SN**: *[String]* Serial number of the hydrophone.
	* **hydrophone_depth**: *[Float > 0]* Depth at which the hydrophone was located in meters.
	* **location_name**: *[string]* Name of the deployment locations.
	* **location_lat**: *[Float]* Latitude of the deployment location in decimal degrees.
	* **location_lon**: *[Float]* Longitude of the deployment location in decimal degrees.
	* **location_water_depth**: *[Float > 0]* Water depth at the deployment location in meters.
	* **deployment_ID**: *[String]* Unique name identifying this particular deployment (e.g. SI-RCAIn-20190410).
	* **deployment_date**: *[String]* Date of deployment. Date format: YYYYMMDDTHHMMSS (e.g. 20190410T150155).
	* **recovery_date**: *[String]* Date of recovery. Date format: YYYYMMDDTHHMMSS (e.g. 20190410T150155).

  Example of deployment_info file:

  .. csv-table:: deployment_info.csv
     :header: "audio_channel_number", "UTC_offset","sampling_frequency","bit_depth","mooring_platform_name","recorder_type","recorder_SN","hydrophone_model","hydrophone_SN","hydrophone_depth","location_name","location_lat","location_lon","location_water_depth","deployment_ID","deployment_date","recovery_date"
   
     1,-8,96000,24,bottom weight,SoundTrap 300,1342218252,SoundTrap 300,1342218252,13.4,Snake Island RCA-In,49.21166667,-123.88405,13.4,SI-RCAIn-20190410,20190410T150155,20190625T051114
 

Running FishSound Finder
------------------------

Now all the data and configuration files are set up and we can run FishSound Finder. It is here assumed that you are using Anaconda on a Windows machine.

* Open Anaconda Navigator
* Under the *Environments* tab on the left, select the python environment in which you installed FishSound Finder (see :ref:`Installation<Installing FishSound Finder>`Installation section),
  and start a Terminal. 
  
  .. image:: _static/open_terminal.png
     :scale: 70 %
  
* Change the current directory to *Tutorial*. 

  .. code-block:: console
  
     $ cd C:\Users\xavier.mouy\Desktop\Tutorial

* Start FishSound Finder to process the audio files that are in the *data* folder. 

  .. code-block:: console
  
     $ fishsound_finder ".\data" ".\results" ".\model\config.yaml" ".\model\RF50_model_20201208T223420.sav" -d".\data\deployment_info.csv" -e".wav" -f -r -p


* **Alternative**: If you don't want to type the input arguments every time, you can also create a text file with all the input arguments (one per line)
  and run FishSound Finder using the @ command pointing to that text file. 
  
  *args_file.txt* (saved in the *Tutorial* folder):
  
  
  .. code-block:: console
  
     .\data
     .\results 
     .\model\config.yaml 
     .\model\RF50_model_20201208T223420.sav 
     -d.\data\deployment_info.csv 
     -e.wav 
     -f 
     -r 
     -p 
  
  Now running FishSound Finder using *args_file.txt*.
    
  .. code-block:: console
  
     $ fishsound_finder @args_file.txt


* The console should now display the files being processed and the processing steps in progress.

  .. code-block:: console
  
     1/1: .\data\67674121.181017060806.wav
     Spectrogram
     Warning: FFT size automatically adjusted to 4096 samples (original size: 4094 )
     Denoise
     Detector
     Measurements
     Classification
     Deleted observations (due to NaNs): 0
     Saving
     Executed in 15.3466 seconds
     All files processed in 15.3476 seconds

* Once FishSound Finder has finished running, all the detection results are written in the *results* folder. In this case,
  it created the default netCDF4 file (*67674121.181017060806.wav.nc*), a Raven file (*67674121.181017060806.wav.chan1.Table.1.selections.txt*)
  and a PAMlab file (*67674121.181017060806.wav annotations.log*).  
  
  .. image:: _static/results.png
     :scale: 70 %
   
  Here we only used a single audio file, but note that FishSound Finder will process all audio files located in the data folder.
   

Reviewing the processing logs
-------------------------

It is important to review the processing logs once FishSound Finder has finished running to ensure there was no errors.
Two log files are automatically created in the *results* folder:

* **errors_log.txt**: Lists all errors that occurred. An empty file indicates no errors occurred.
* **full_log.txt**: Lists all the information displayed during the processing (including processing times and warning messages).


Analyzing the detection results
-------------------------------

Detection results from FishSound Finder can be analyzed using the bioacoustics software Raven, PAMlab, or
libraries such as the ecosound. With the example of the fish detector from British-Columbia, all detections are 
saved in the output files and are labelled either **FS**, for a fish sound, or **NN**, for noise. Each detections has
a classification confidence value (comprised between 0 and 1) which can be used to make the detector more or less sensitive
depending on the application.

With PAMlab
***********

To visualize the detection results in PAMlab:

* Create a subfolder called *annotations* in the *data* folder
* Copy the file *67674121.181017060806.wav annotations.log* in the *annotations* folder.
* Open the audio file in PAMlab.
* Detection boxes should automatically appear (FS: fish sounds, NN: noise).


.. image:: _static/pamlab_no_detections.png
   :scale: 70 %
   
.. image:: _static/pamlab_with_detections.png
   :scale: 70 %
   
   
With Raven
**********

To visualize the detection results in Raven:

* Open the audio file in Raven
* In the **File** menu, select "**Open Sound Selection Table...**", then select the file *67674121.181017060806.wav.chan1.Table.1.selections.txt*
  from the *results* folder.
* Detection boxes should automatically appear. Notice the confidence value in the selection table. 

.. image:: _static/raven_with_detections.png
   :scale: 70 %


With ecosound
*************

Here are some code snippets that can be used to analyze the detection results with ecosound. While ecosound can import
data from Raven and PAMlab tables, it is typically better to import the netCDF4 file, as it contains all the metadata
and measurements for each detections.

Example 1: Display a summary of the detections
______________________________________________
   
   .. code-block:: python
   
      from ecosound.core.measurement import Measurement
      detection_file = r".\results\67674121.181017060806.wav.nc"
      detec = Measurement()
      detec.from_netcdf(detection_file)
      detec.summary()
   
   Resulting summary table:
   
   .. code-block:: python  

      label_class        FS  NN  Total
      deployment_ID                   
      SI-RCAIn-20190410  22  50     72
      Total              22  50     72

   
Example 2: Display detections on top the spectrogram and waveform
_________________________________________________________________
   
   .. code-block:: python
      
      from ecosound.core.measurement import Measurement
      from ecosound.core.audiotools import Sound
      from ecosound.core.spectrogram import Spectrogram
      from ecosound.visualization.grapher_builder import GrapherFactory

      # load detections
      detection_file = r".\results\67674121.181017060806.wav.nc"
      detec = Measurement()
      detec.from_netcdf(detection_file)

      # load audio file (first 15 sec only)
      audio_file = r".\data\67674121.181017060806.wav"
      sound = Sound(audio_file)
      sound.read(channel=0, chunk=[0, 15], unit='sec', detrend=True)

      # Calculate spectrogram
      frame = 3000
      nfft = 4096
      step = 500
      window_type = 'hann'
      spectro = Spectrogram(frame, window_type, nfft, step, sound.waveform_sampling_frequency, unit='samp')
      spectro.compute(sound, dB=True)

      # Generate plot with waveform and spectrogram
      graph = GrapherFactory('SoundPlotter', title='Recording', frequency_max=1000)
      graph.add_data(sound) # add waveform data
      graph.add_data(spectro) # add spectrogram
      graph.add_annotation(detec, panel=0, color='green', label='Detections') # overlay detections on waveform plot
      graph.add_annotation(detec, panel=1, color='green', label='Detections') # overlay detections on spectrogram plot
      graph.colormap = 'binary'
      graph.show()

 
   Resulting graph:

   .. image:: _static/ecosound_sepctrogram.png
      :scale: 70 %

Example 3: Visualize the distribution of the detections' peak frequency
_______________________________________________________________________
   
   .. code-block:: python
   
      from ecosound.core.measurement import Measurement
      import seaborn as sns
	  
      # load detections
      detection_file = r".\results\67674121.181017060806.wav.nc"
      detec = Measurement()
      detec.from_netcdf(detection_file)
	  
      # create violin plot
      fig = sns.violinplot(y=detec.data['label_class'], x=detec.data['freq_peak'])
      fig.set(xlabel='Detections peak frequency (Hz)', ylabel='Detections class')
	  
   Resulting graph:
   
   .. image:: _static/violin_graph.png
      :scale: 90 %